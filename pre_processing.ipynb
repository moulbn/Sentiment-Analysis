{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numeric-canada",
   "metadata": {},
   "source": [
    "# Movie Reviews - Setup - AITAMALIK Amine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-charger",
   "metadata": {},
   "source": [
    "The purpose of the following code is to extract and split the reviews into a list of words which we will then use to create a vocabulary along with a Bag of Words representation. This will allow us to easily classify the reviews.\n",
    "We here have 3 types of data:\n",
    "- The first one is with a 1000 word vocabulary\n",
    "- The second has a vocabulary of a 2000 words\n",
    "- The last one has a vocabulary of 1000 words but the words have been stemmed and stopwords are discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-headquarters",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gorgeous-verse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77727fbc",
   "metadata": {},
   "source": [
    "# PART 1 - NORMAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-elements",
   "metadata": {},
   "source": [
    "### Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "million-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = \"!#$%&()''*+-/.:;?@[]{}|^_`~<>=\\\"\" # all punctuation we discard\n",
    "TABLE = str.maketrans(PUNCT, \" \" * len(PUNCT)) # replace punctuation by space\n",
    "\n",
    "def read_document(filename):\n",
    "    f = open(filename, encoding=\"utf-8\") # specify encoding to avoid unreadable documents\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    text = text.lower() # all words to lowercase\n",
    "    text = text.translate(TABLE)\n",
    "    words = text.split() # separate the document into list of words\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-efficiency",
   "metadata": {},
   "source": [
    "#### Test basic word splitting on one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-tomorrow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'went', 'and', 'saw', 'this', 'movie', 'last', 'night', 'after', 'being', 'coaxed', 'to', 'by', 'a', 'few', 'friends', 'of', 'mine', 'i', 'll', 'admit', 'that', 'i', 'was', 'reluctant', 'to', 'see', 'it', 'because', 'from', 'what', 'i', 'knew', 'of', 'ashton', 'kutcher', 'he', 'was', 'only', 'able', 'to', 'do', 'comedy', 'i', 'was', 'wrong', 'kutcher', 'played', 'the', 'character', 'of', 'jake', 'fischer', 'very', 'well,', 'and', 'kevin', 'costner', 'played', 'ben', 'randall', 'with', 'such', 'professionalism', 'the', 'sign', 'of', 'a', 'good', 'movie', 'is', 'that', 'it', 'can', 'toy', 'with', 'our', 'emotions', 'this', 'one', 'did', 'exactly', 'that', 'the', 'entire', 'theater', 'which', 'was', 'sold', 'out', 'was', 'overcome', 'by', 'laughter', 'during', 'the', 'first', 'half', 'of', 'the', 'movie,', 'and', 'were', 'moved', 'to', 'tears', 'during', 'the', 'second', 'half', 'while', 'exiting', 'the', 'theater', 'i', 'not', 'only', 'saw', 'many', 'women', 'in', 'tears,', 'but', 'many', 'full', 'grown', 'men', 'as', 'well,', 'trying', 'desperately', 'not', 'to', 'let', 'anyone', 'see', 'them', 'crying', 'this', 'movie', 'was', 'great,', 'and', 'i', 'suggest', 'that', 'you', 'go', 'see', 'it', 'before', 'you', 'judge']\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(\"aclImdb/test/pos\"):\n",
    "    path = \"aclImdb/test/pos/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-shape",
   "metadata": {},
   "source": [
    "## I. Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = collections.Counter()\n",
    "\n",
    "# positive reviews\n",
    "for f in os.listdir(\"aclImdb/smalltrain/pos\"):\n",
    "    path = \"aclImdb/smalltrain/pos/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    vocabulary.update(words) # count words and add to vocabulary\n",
    "    \n",
    "# negative reviews\n",
    "for f in os.listdir(\"aclImdb/smalltrain/neg\"):\n",
    "    path = \"aclImdb/smalltrain/neg/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    vocabulary.update(words) # count words and add to vocabulary\n",
    "\n",
    "# Save vocabulary in \"vocabulary.txt\" file\n",
    "f = open(\"vocabulary.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for word, count in vocabulary.most_common(1000): # 1000 most common words\n",
    "    print(word, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affected-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(filename):\n",
    "    f = open(filename, encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    words = text.split()\n",
    "    \n",
    "    # Create index for each word\n",
    "    voc = {}\n",
    "    index = 0\n",
    "    for word in words:\n",
    "        voc[word] = index\n",
    "        index += 1\n",
    "    \n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "central-economy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0), ('a', 1), ('and', 2), ('of', 3), ('to', 4)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = load_vocabulary(\"vocabulary.txt\")\n",
    "list(vocabulary.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-theology",
   "metadata": {},
   "source": [
    "## II. Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = \"!#$%&()''*+-/.:;?@[]{}|^_`~<>=\\\"\" # all punctuation we discard\n",
    "TABLE = str.maketrans(PUNCT, \" \" * len(PUNCT)) # replace punctuation by space\n",
    "\n",
    "def read_document_bow(filename, voc):\n",
    "    f = open(filename, encoding=\"utf-8\") # specify encoding to avoid unreadable documents\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    text = text.lower() # all words to lowercase\n",
    "    text = text.translate(TABLE)\n",
    "    words = text.split() # separate the document into list of words\n",
    "    \n",
    "    # Bag of Words\n",
    "    bow = np.zeros(len(voc))\n",
    "    for word in words:\n",
    "        if word in voc:\n",
    "            index = voc[word]\n",
    "            bow[index] += 1\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-layout",
   "metadata": {},
   "source": [
    "#### Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "# Positive Reviews\n",
    "for f in os.listdir(\"aclImdb/train/pos\"):\n",
    "    path = \"aclImdb/train/pos/\" + f\n",
    "    bow = read_document_bow(path, vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative Reviews\n",
    "for f in os.listdir(\"aclImdb/train/neg\"):\n",
    "    path = \"aclImdb/train/neg/\" + f\n",
    "    bow = read_document_bow(path, vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(0)\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"train.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-papua",
   "metadata": {},
   "source": [
    "#### Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "# Positive Reviews\n",
    "for f in os.listdir(\"aclImdb/test/pos\"):\n",
    "    path = \"aclImdb/test/pos/\" + f\n",
    "    bow = read_document_bow(path, vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative Reviews\n",
    "for f in os.listdir(\"aclImdb/test/neg\"):\n",
    "    path = \"aclImdb/test/neg/\" + f\n",
    "    bow = read_document_bow(path, vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(0)\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"test.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81585a",
   "metadata": {},
   "source": [
    "# PART 2 - BIGGER VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f33c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_vocabulary = collections.Counter()\n",
    "\n",
    "# positive reviews\n",
    "for f in os.listdir(\"aclImdb/smalltrain/pos\"):\n",
    "    path = \"aclImdb/smalltrain/pos/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    bigger_vocabulary.update(words) # count words and add to bigger_vocabulary\n",
    "    \n",
    "# negative reviews\n",
    "for f in os.listdir(\"aclImdb/smalltrain/neg\"):\n",
    "    path = \"aclImdb/smalltrain/neg/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    bigger_vocabulary.update(words) # count words and add to bigger_vocabulary\n",
    "\n",
    "# Save bigger_vocabulary in \"bigger_vocabulary.txt\" file\n",
    "f = open(\"bigger_vocabulary.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for word, count in bigger_vocabulary.most_common(2000): # 2000 most common words\n",
    "    print(word, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceb67e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length - First Vocabulary: 1000\n",
      "Length - Bigger vocabulary: 2000\n"
     ]
    }
   ],
   "source": [
    "bigger_vocabulary = load_vocabulary(\"bigger_vocabulary.txt\")\n",
    "print(f\"Length - First Vocabulary: {len(vocabulary)}\")\n",
    "print(f\"Length - Bigger vocabulary: {len(bigger_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed32da",
   "metadata": {},
   "source": [
    "## II. Bag of Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010412c",
   "metadata": {},
   "source": [
    "#### Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "# Positive Reviews\n",
    "for f in os.listdir(\"aclImdb/train/pos\"):\n",
    "    path = \"aclImdb/train/pos/\" + f\n",
    "    bow = read_document_bow(path, bigger_vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative Reviews\n",
    "for f in os.listdir(\"aclImdb/train/neg\"):\n",
    "    path = \"aclImdb/train/neg/\" + f\n",
    "    bow = read_document_bow(path, bigger_vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(0)\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"big_voc_train.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc11397",
   "metadata": {},
   "source": [
    "#### Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2511e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "# Positive Reviews\n",
    "for f in os.listdir(\"aclImdb/test/pos\"):\n",
    "    path = \"aclImdb/test/pos/\" + f\n",
    "    bow = read_document_bow(path, bigger_vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative Reviews\n",
    "for f in os.listdir(\"aclImdb/test/neg\"):\n",
    "    path = \"aclImdb/test/neg/\" + f\n",
    "    bow = read_document_bow(path, bigger_vocabulary)\n",
    "    documents.append(bow)\n",
    "    labels.append(0)\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"big_voc_test.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e2ddb",
   "metadata": {},
   "source": [
    "# PART 3  - Without Stopwords - Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33d45ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'across', 'after']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"aclImdb/\")\n",
    "sw_path = \"aclImdb/stopwords.txt\"\n",
    "stopwords = read_document(sw_path)\n",
    "\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf62a34",
   "metadata": {},
   "source": [
    "## I. Stem and Discard Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "018bfac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_stem_sw(filename):\n",
    "    words = read_document(filename)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    words_without_sw = [i for i in words if i not in stopwords]\n",
    "    \n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(word) for word in words_without_sw]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45483bde",
   "metadata": {},
   "source": [
    "#### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "789224f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'high', 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'i', 'm', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'isn', 't']\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(\"aclImdb/smalltrain/pos\"):\n",
    "    path = \"aclImdb/smalltrain/pos/\" + f\n",
    "    words = read_document(path)\n",
    "    \n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5662b",
   "metadata": {},
   "source": [
    "#### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "949ae0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwel', 'high', 'cartoon', 'comedi', 'ran', 'time', 'program', 'school', 'life,', 'teacher', '35', 'year', 'teach', 'profess', 'lead', 'believ', 'bromwel', 'high', 's', 'satir', 'closer', 'realiti', 'teacher', 'scrambl', 'surviv', 'financially,', 'insight', 'student', 'right', 'pathet', 'teacher', 'pomp,', 'petti', 'situation,', 'remind', 'school', 'knew', 'student', 'saw', 'episod', 'student', 'repeatedli', 'tri', 'burn', 'school,', 'immedi', 'recal', 'high', 'classic', 'line', 'inspector', 'm', 'sack', 'teacher', 'student', 'welcom', 'bromwel', 'high', 'expect', 'adult', 'age', 'think', 'bromwel', 'high', 'far', 'fetch', 'piti', 'isn', 't']\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(\"aclImdb/smalltrain/pos\"):\n",
    "    path = \"aclImdb/smalltrain/pos/\" + f\n",
    "    words = read_document_stem_sw(path)\n",
    "    \n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299644c",
   "metadata": {},
   "source": [
    "## II. New Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_stem_sw = collections.Counter()\n",
    "\n",
    "# positive reviews\n",
    "for f in os.listdir(\"aclImdb/smalltrain/pos\"):\n",
    "    path = \"aclImdb/smalltrain/pos/\" + f\n",
    "    words = read_document_stem_sw(path)\n",
    "    \n",
    "    vocabulary_stem_sw.update(words) # count words and add to vocabulary\n",
    "    \n",
    "# negative reviews\n",
    "for f in os.listdir(\"aclImdb/smalltrain/neg\"):\n",
    "    path = \"aclImdb/smalltrain/neg/\" + f\n",
    "    words = read_document_stem_sw(path)\n",
    "    \n",
    "    vocabulary_stem_sw.update(words) # count words and add to vocabulary\n",
    "    \n",
    "# Save vocabulary in \"vocabulary_stem_sw.txt\" file\n",
    "f = open(\"vocabulary_stem_sw.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for word, count in vocabulary_stem_sw.most_common(1000): # 1000 most common words\n",
    "    print(word, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5032f610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 0), ('s', 1), ('movi', 2), ('film', 3), ('t', 4)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_stem_sw = load_vocabulary(\"vocabulary_stem_sw.txt\")\n",
    "list(vocabulary_stem_sw.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992aa58",
   "metadata": {},
   "source": [
    "## III. Bag of Words - Train and Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481216a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_stem_sw_bow(filename, voc):\n",
    "    words = read_document(filename)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    words_without_sw = [i for i in words if i not in stopwords]\n",
    "    \n",
    "    #Stemming\n",
    "    stemmed = [ps.stem(word) for word in words_without_sw]\n",
    "    \n",
    "    # Bag of Words\n",
    "    bow = np.zeros(len(voc))\n",
    "    for word in words:\n",
    "        if word in voc:\n",
    "            index = voc[word]\n",
    "            bow[index] += 1\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffb6a5",
   "metadata": {},
   "source": [
    "#### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0420e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "# Positive Reviews\n",
    "for f in os.listdir(\"aclImdb/train/pos\"):\n",
    "    path = \"aclImdb/train/pos/\" + f\n",
    "    bow = read_document_stem_sw_bow(path, vocabulary_stem_sw)\n",
    "    documents.append(bow)\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative Reviews\n",
    "for f in os.listdir(\"aclImdb/train/neg\"):\n",
    "    path = \"aclImdb/train/neg/\" + f\n",
    "    bow = read_document_stem_sw_bow(path, vocabulary_stem_sw)\n",
    "    documents.append(bow)\n",
    "    labels.append(0)\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"train_stem_sw.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7f9a8",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "# Positive Reviews\n",
    "for f in os.listdir(\"aclImdb/test/pos\"):\n",
    "    path = \"aclImdb/test/pos/\" + f\n",
    "    bow = read_document_stem_sw_bow(path, vocabulary_stem_sw)\n",
    "    documents.append(bow)\n",
    "    labels.append(1)\n",
    "\n",
    "# Negative Reviews\n",
    "for f in os.listdir(\"aclImdb/test/neg\"):\n",
    "    path = \"aclImdb/test/neg/\" + f\n",
    "    bow = read_document_stem_sw_bow(path, vocabulary_stem_sw)\n",
    "    documents.append(bow)\n",
    "    labels.append(0)\n",
    "\n",
    "X = np.stack(documents)\n",
    "Y = np.array(labels)\n",
    "\n",
    "data = np.concatenate([X, Y[:, None]], 1)\n",
    "\n",
    "np.savetxt(\"test_stem_sw.txt.gz\", data) # Save into a file / .gz compresses the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b6436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
